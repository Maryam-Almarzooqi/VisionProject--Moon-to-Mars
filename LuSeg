# LuSeg Model Implementations with ResNet-50 Backbone
#
# Purpose:
#   This script implements multiple variants of the LuSeg (Luo's Segmentation) architecture
#   for semantic segmentation tasks using RGB and thermal/depth imagery. The models include:
#     - LuSeg_T: Teacher model with full cross-modality fusion
#     - LuSeg_S: Student model with simplified fusion
#     - LuSeg_RGB: RGB-only variant for single-modality segmentation
#     - LuSeg_Depth: Depth/Thermal-only variant for single-modality segmentation
#
# Architecture Overview:
#   - Encoder: ResNet-50 pretrained on ImageNet (dual encoders for RGB and thermal)
#   - Decoder: U-Net style with upsampling blocks and skip connections
#   - Fusion: Multi-level fusion modules combining RGB and thermal features
#   - Loss: Includes contrastive loss (NTXentLoss) for feature learning
#
# Key Features:
#   - Transfer learning from ImageNet weights
#   - Squeeze-and-Excitation (SE) blocks for channel attention
#   - Multi-scale feature fusion at decoder stages
#   - Support for both single and dual-modality inputs

import torch
import torch.nn as nn
import torchvision.models as models
import torch.nn.functional as F


# ============================================================================
# AUXILIARY MODULES (Dependencies for LuSeg)
# ============================================================================

class NTXentLoss(nn.Module):
    """
    Normalized Temperature-scaled Cross Entropy Loss (NT-Xent Loss)
    
    Used for contrastive learning to pull positive pairs closer and push
    negative pairs apart in the embedding space. Commonly used in SimCLR
    and other self-supervised learning frameworks.
    
    Args:
        temperature: Scaling factor for similarity scores (default=0.5)
                    Lower values make the model more confident in predictions
    """
    def __init__(self, temperature=0.5):
        super(NTXentLoss, self).__init__()
        self.temperature = temperature
        self.eps = 1e-6  # Small epsilon to prevent division by zero

    def forward(self, x1, x2):
        """
        Computes contrastive loss between two sets of embeddings.
        
        Args:
            x1: First set of embeddings (batch_size, embedding_dim)
            x2: Second set of embeddings (batch_size, embedding_dim)
            
        Returns:
            Scalar loss value
        """
        # Normalize embeddings to unit sphere (L2 normalization)
        x1 = F.normalize(x1, dim=1)
        x2 = F.normalize(x2, dim=1)

        # Compute similarity matrix: cosine similarity scaled by temperature
        sim_matrix = torch.matmul(x1, x2.T) / self.temperature
        
        # Subtract max for numerical stability (prevents overflow in exp)
        sim_matrix = sim_matrix - torch.max(sim_matrix, dim=-1, keepdim=True)[0]

        # Create labels: diagonal elements are positive pairs
        labels = torch.arange(x1.size(0)).to(x1.device)
        
        # Compute cross-entropy loss
        loss = nn.CrossEntropyLoss()(sim_matrix, labels)

        return loss


class SE_fz(nn.Module):
    """
    Squeeze-and-Excitation Block (Channel Attention Mechanism)
    
    Adaptively recalibrates channel-wise feature responses by explicitly
    modeling interdependencies between channels. The block learns to
    emphasize informative features and suppress less useful ones.
    
    Architecture:
        1. Squeeze: Global average pooling to create channel descriptor
        2. Excitation: Two FC layers to capture channel dependencies
        3. Scale: Multiply input by learned channel weights
    
    Args:
        in_channels: Number of input channels
        med_channels: Number of channels in the bottleneck (reduction ratio)
    """
    def __init__(self, in_channels, med_channels):
        super(SE_fz, self).__init__()
        # Squeeze operation: Global spatial information into channel descriptor
        self.average = nn.AdaptiveAvgPool2d(1)
        
        # Excitation operation: Two FC layers with bottleneck architecture
        self.fc1 = nn.Linear(in_channels, med_channels)  # Dimension reduction
        self.bn1 = nn.BatchNorm1d(med_channels)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(med_channels, in_channels)  # Dimension restoration
        self.sg = nn.Sigmoid()  # Output channel weights in [0, 1]

    def forward(self, input):
        """
        Args:
            input: Feature map (batch, channels, height, width)
            
        Returns:
            Recalibrated feature map (same shape as input)
        """
        # Squeeze: (B, C, H, W) -> (B, C, 1, 1)
        x = self.average(input)
        
        # Flatten spatial dimensions: (B, C, 1, 1) -> (B, C)
        x = x.squeeze(2).squeeze(2)
        
        # Excitation: Learn channel importance weights
        x = self.fc1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.sg(x)  # Sigmoid activation for gating
        
        # Restore spatial dimensions: (B, C) -> (B, C, 1, 1)
        x = x.unsqueeze(2).unsqueeze(3)
        
        # Scale: Element-wise multiplication with original input
        out = torch.mul(input, x)
        return out


class upbolckV1(nn.Module):
    """
    Upsampling Block with Residual Connection and SE Attention
    
    This module serves as a decoder building block in the U-Net architecture.
    It combines:
        - Three convolutional layers for feature refinement
        - Residual/skip connection for gradient flow
        - SE block for channel attention
        - Transpose convolution for spatial upsampling
    
    Args:
        cin: Number of input channels
        cout: Number of output channels (after upsampling)
    """
    def __init__(self, cin, cout):
        super().__init__()
        # Main convolutional path (3 layers with BN and ReLU)
        self.conv1 = nn.Conv2d(cin, cin // 2, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(cin // 2)
        self.relu1 = nn.ReLU(inplace=True)
        
        self.conv2 = nn.Conv2d(cin // 2, cin // 2, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(cin // 2)
        self.relu2 = nn.ReLU(inplace=True)
        
        self.conv3 = nn.Conv2d(cin // 2, cin // 2, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(cin // 2)
        self.relu3 = nn.ReLU(inplace=True)

        # Shortcut/Residual connection (1x1 conv for channel matching)
        self.shortcutconv = nn.Conv2d(cin, cin // 2, kernel_size=1, stride=1)
        self.shortcutbn = nn.BatchNorm2d(cin // 2)
        self.shortcutrelu = nn.ReLU(inplace=True)

        # Squeeze-and-Excitation block for channel attention
        self.se = SE_fz(in_channels=cin // 2, med_channels=cin // 4)

        # Upsampling via Transpose Convolution (doubles spatial dimensions)
        self.transconv = nn.ConvTranspose2d(cin // 2, cout, kernel_size=2, stride=2, padding=0, bias=False)
        self.transbn = nn.BatchNorm2d(cout)
        self.transrelu = nn.ReLU(inplace=True)

    def forward(self, x):
        """
        Args:
            x: Input feature map (batch, cin, H, W)
            
        Returns:
            Upsampled feature map (batch, cout, 2*H, 2*W)
        """
        # Main path: Three convolutional layers
        fusion = self.conv1(x)
        fusion = self.bn1(fusion)
        fusion = self.relu1(fusion)
        
        fusion = self.conv2(fusion)
        fusion = self.bn2(fusion)
        fusion = self.relu2(fusion)
        
        fusion = self.conv3(fusion)
        fusion = self.bn3(fusion)
        fusion = self.relu3(fusion)

        # Shortcut connection (residual)
        sc = self.shortcutconv(x)
        sc = self.shortcutbn(sc)
        sc = self.shortcutrelu(sc)

        # Add residual and apply SE attention
        fusion = fusion + sc
        fusion = self.se(fusion)
        
        # Upsample via transpose convolution
        fusion = self.transconv(fusion)
        fusion = self.transbn(fusion)
        fusion = self.transrelu(fusion)
        
        return fusion


class FusionV10(nn.Module):
    """
    Advanced Cross-Modality Fusion Module (Version 1.0)
    
    Fuses RGB and depth/thermal features at decoder stages using:
        - Subtraction to capture modality differences
        - Multiplicative gating for adaptive feature weighting
        - Concatenation followed by 1x1 convolution for feature integration
    
    This module can optionally produce intermediate segmentation outputs
    for deep supervision during training.
    
    Args:
        in_channel: Number of input channels from each modality
        n_class: Number of segmentation classes
    """
    def __init__(self, in_channel, n_class):
        super(FusionV10, self).__init__()
        # Flag to determine if intermediate segmentation is needed
        self.seg = in_channel != n_class
        
        if self.seg:
            # 1x1 convolutions for intermediate segmentation predictions
            self.rgbsegconv = nn.Conv2d(in_channels=in_channel, out_channels=n_class, 
                                       kernel_size=1, stride=1, padding=0)
            self.depthsegconv = nn.Conv2d(in_channels=in_channel, out_channels=n_class, 
                                         kernel_size=1, stride=1, padding=0)
            # Feedback path: Convert segmentation back to feature space
            self.feedback = nn.Conv2d(in_channels=n_class, out_channels=in_channel, kernel_size=1)

        # Feature refinement on the difference map
        self.feature_conv1 = nn.Conv2d(in_channels=n_class, out_channels=n_class, 
                                      kernel_size=3, stride=1, padding=1)
        
        # Final fusion: Concatenate [rgb, rgb*gating, gating] and reduce channels
        self.fusion_conv = nn.Conv2d(in_channels=3 * in_channel, out_channels=in_channel, kernel_size=1)

    def forward(self, rgb, depth):
        """
        Args:
            rgb: RGB features (batch, in_channel, H, W)
            depth: Depth/Thermal features (batch, in_channel, H, W)
            
        Returns:
            rgb_seg: Intermediate RGB segmentation
            depth_add: Refined depth features with residual
            fusion_result: Fused RGB-Depth features
        """
        # Compute modality difference (captures complementary information)
        sub_fusion = depth - rgb

        if self.seg:
            # Generate intermediate segmentation maps
            rgb_seg = self.rgbsegconv(rgb)
            sub_fusion = self.depthsegconv(sub_fusion)
        else:
            rgb_seg = rgb

        # Refine depth features with residual connection
        depth_add = self.feature_conv1(sub_fusion)
        depth_add = depth_add + sub_fusion  # Residual

        if self.seg:
            # Convert refined segmentation back to feature space (feedback)
            depth_add_feedback = self.feedback(depth_add)
        else:
            depth_add_feedback = depth_add

        # Fusion: Concatenate RGB, gated RGB, and depth features
        # rgb * depth_add_feedback: Multiplicative gating (attention mechanism)
        fusion_result = torch.cat((rgb, rgb * depth_add_feedback, depth_add_feedback), dim=1)
        fusion_result = self.fusion_conv(fusion_result)

        return rgb_seg, depth_add, fusion_result


class FusionV1(nn.Module):
    """
    Simple Additive Fusion Module (Version 1)
    
    Basic fusion strategy that simply adds depth and RGB features.
    Used at deeper decoder levels where features are more abstract.
    
    No learnable parameters - pure element-wise addition.
    """
    def __init__(self):
        super(FusionV1, self).__init__()

    def forward(self, depth, rgb):
        """
        Args:
            depth: Depth features (batch, channels, H, W)
            rgb: RGB features (batch, channels, H, W)
            
        Returns:
            Fused features (element-wise sum)
        """
        fusion = depth + rgb
        return fusion


# ============================================================================
# LUSEG MODEL CLASSES - UPDATED TO RESNET50
# ============================================================================

class LuSeg_T(nn.Module):
    """
    LuSeg Teacher Model (Full Dual-Encoder Architecture with Cross-Modality Fusion)
    
    This is the complete teacher model that processes both RGB and thermal/depth
    images through separate encoders and fuses them at multiple decoder stages.
    
    Architecture:
        - Dual ResNet-50 encoders (one for RGB, one for thermal)
        - Separate decoders for each modality
        - Multi-level fusion modules (FusionV1 and FusionV10)
        - Deep supervision with intermediate segmentation outputs
    
    Input:
        4-channel tensor: [RGB (3 channels) | Thermal/Depth (1 channel)]
        Shape: (batch, 4, H, W)
    
    Outputs (10 tensors for deep supervision):
        - depth_en_out: Depth encoder features (flattened)
        - rgb_en_out: RGB encoder features (flattened)
        - depth_decoder_out_1: Final depth segmentation
        - rgb_decoder_out_1: Final fused segmentation
        - rgb_seg_f1, depth_add_f1: Fusion outputs from decoder level 1
        - rgb_seg_f2, depth_add_f2: Fusion outputs from decoder level 2
        - rgb_seg_f3, depth_add_f3: Fusion outputs from decoder level 3
    
    Args:
        n_class: Number of segmentation classes
    """
    def __init__(self, n_class):
        super(LuSeg_T, self).__init__()
        
        # Load two separate ResNet-50 models pretrained on ImageNet
        resnet_raw_model1 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        resnet_raw_model2 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        self.inplanes = 2048  # Output channel count for ResNet-50 layer4

        # ========== THERMAL/DEPTH ENCODER ==========
        # Modify first conv layer to accept 1-channel input (thermal/depth)
        # Weight initialization: Average RGB weights across channels
        self.encoder_depth_conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.encoder_depth_conv1.weight.data = torch.unsqueeze(
            torch.mean(resnet_raw_model1.conv1.weight.data, dim=1), dim=1
        )
        
        # Remaining encoder layers from ResNet-50
        self.encoder_depth_bn1 = resnet_raw_model1.bn1
        self.encoder_depth_relu = resnet_raw_model1.relu
        self.encoder_depth_maxpool = resnet_raw_model1.maxpool
        self.encoder_depth_layer1 = resnet_raw_model1.layer1  # Output: 256 channels
        self.encoder_depth_layer2 = resnet_raw_model1.layer2  # Output: 512 channels
        self.encoder_depth_layer3 = resnet_raw_model1.layer3  # Output: 1024 channels
        self.encoder_depth_layer4 = resnet_raw_model1.layer4  # Output: 2048 channels

        # ========== RGB ENCODER ==========
        # Standard ResNet-50 encoder for 3-channel RGB input
        self.encoder_rgb_conv1 = resnet_raw_model2.conv1
        self.encoder_rgb_bn1 = resnet_raw_model2.bn1
        self.encoder_rgb_relu = resnet_raw_model2.relu
        self.encoder_rgb_maxpool = resnet_raw_model2.maxpool
        self.encoder_rgb_layer1 = resnet_raw_model2.layer1    # Output: 256 channels
        self.encoder_rgb_layer2 = resnet_raw_model2.layer2    # Output: 512 channels
        self.encoder_rgb_layer3 = resnet_raw_model2.layer3    # Output: 1024 channels
        self.encoder_rgb_layer4 = resnet_raw_model2.layer4    # Output: 2048 channels

        # ========== RGB DECODER ==========
        # Skip connection transform for early features (64 -> 128 channels)
        self.skip_tranform_rgb = nn.Conv2d(in_channels=64, out_channels=128, 
                                          kernel_size=3, stride=1, padding=1)
        
        # U-Net style decoder with upsampling blocks
        self.deconv5_rgb = upbolckV1(cin=2048, cout=1024)  # 8x8 -> 16x16
        self.deconv4_rgb = upbolckV1(cin=1024, cout=512)   # 16x16 -> 32x32
        self.deconv3_rgb = upbolckV1(cin=512, cout=256)    # 32x32 -> 64x64
        self.deconv2_rgb = upbolckV1(cin=256, cout=128)    # 64x64 -> 128x128
        self.deconv1_rgb = upbolckV1(cin=128, cout=n_class) # 128x128 -> 256x256

        # ========== DEPTH DECODER ==========
        self.skip_tranform_depth = nn.Conv2d(in_channels=64, out_channels=128, 
                                            kernel_size=3, stride=1, padding=1)
        
        self.deconv5_depth = upbolckV1(cin=2048, cout=1024)
        self.deconv4_depth = upbolckV1(cin=1024, cout=512)
        self.deconv3_depth = upbolckV1(cin=512, cout=256)
        self.deconv2_depth = upbolckV1(cin=256, cout=128)
        self.deconv1_depth = upbolckV1(cin=128, cout=n_class)

        # ========== MULTI-LEVEL FUSION MODULES ==========
        # Simple additive fusion for deep features (layers 5 and 4)
        self.fusion5 = FusionV1()
        self.fusion4 = FusionV1()
        
        # Advanced fusion with gating for shallow features (layers 3, 2, 1)
        self.fusion3 = FusionV10(in_channel=256, n_class=n_class)
        self.fusion2 = FusionV10(in_channel=128, n_class=n_class)
        self.fusion1 = FusionV10(in_channel=n_class, n_class=n_class)

    def forward(self, input):
        """
        Forward pass through the teacher model.
        
        Args:
            input: 4-channel tensor (batch, 4, H, W)
                   Channels 0-2: RGB
                   Channel 3: Thermal/Depth
                   
        Returns:
            Tuple of 10 tensors for deep supervision and contrastive learning
        """
        # Split input into RGB and thermal/depth
        rgb = input[:, :3]    # First 3 channels
        depth = input[:, 3:]  # Last channel

        # ========== DEPTH ENCODER ==========
        depth = self.encoder_depth_conv1(depth)
        depth = self.encoder_depth_bn1(depth)
        depth = self.encoder_depth_relu(depth)
        skip_d_1 = depth  # Save for skip connection (64 channels, H/2 x W/2)
        
        depth = self.encoder_depth_maxpool(depth)
        depth = self.encoder_depth_layer1(depth)
        skip_d_2 = depth  # 256 channels, H/4 x W/4
        
        depth = self.encoder_depth_layer2(depth)
        skip_d_3 = depth  # 512 channels, H/8 x W/8
        
        depth = self.encoder_depth_layer3(depth)
        skip_d_4 = depth  # 1024 channels, H/16 x W/16
        
        depth = self.encoder_depth_layer4(depth)  # 2048 channels, H/32 x W/32
        depth_en_out = depth.view(depth.size(0), -1)  # Flatten for contrastive loss

        # ========== DEPTH DECODER (Independent Path) ==========
        # Decoder with skip connections from depth encoder
        depth_decoder_out_5 = self.deconv5_depth(depth) + skip_d_4
        depth_decoder_out_4 = self.deconv4_depth(depth_decoder_out_5) + skip_d_3
        depth_decoder_out_3 = self.deconv3_depth(depth_decoder_out_4) + skip_d_2
        depth_decoder_out_2 = self.deconv2_depth(depth_decoder_out_3) + self.skip_tranform_depth(skip_d_1)
        depth_decoder_out_1 = self.deconv1_depth(depth_decoder_out_2)

        # ========== RGB ENCODER ==========
        rgb = self.encoder_rgb_conv1(rgb)
        rgb = self.encoder_rgb_bn1(rgb)
        rgb = self.encoder_rgb_relu(rgb)
        skip_r_1 = rgb  # 64 channels, H/2 x W/2
        
        rgb = self.encoder_rgb_maxpool(rgb)
        rgb = self.encoder_rgb_layer1(rgb)
        skip_r_2 = rgb  # 256 channels, H/4 x W/4
        
        rgb = self.encoder_rgb_layer2(rgb)
        skip_r_3 = rgb  # 512 channels, H/8 x W/8
        
        rgb = self.encoder_rgb_layer3(rgb)
        skip_r_4 = rgb  # 1024 channels, H/16 x W/16
        
        rgb = self.encoder_rgb_layer4(rgb)  # 2048 channels, H/32 x W/32
        rgb_en_out = rgb.view(rgb.size(0), -1)  # Flatten for contrastive loss

        # ========== RGB DECODER (With Cross-Modality Fusion) ==========
        # Level 5: Additive fusion at deepest level
        rgb_decoder_out_5 = self.deconv5_rgb(rgb) + skip_r_4
        rgb_decoder_out_5 = self.fusion5(rgb_decoder_out_5, depth_decoder_out_5)

        # Level 4: Additive fusion
        rgb_decoder_out_4 = self.deconv4_rgb(rgb_decoder_out_5) + skip_r_3
        rgb_decoder_out_4 = self.fusion4(rgb_decoder_out_4, depth_decoder_out_4)

        # Level 3: Advanced fusion with intermediate segmentation
        rgb_decoder_out_3 = self.deconv3_rgb(rgb_decoder_out_4) + skip_r_2
        rgb_seg_f3, depth_add_f3, rgb_decoder_out_3 = self.fusion3(rgb_decoder_out_3, depth_decoder_out_3)

        # Level 2: Advanced fusion with intermediate segmentation
        rgb_decoder_out_2 = self.deconv2_rgb(rgb_decoder_out_3) + self.skip_tranform_rgb(skip_r_1)
        rgb_seg_f2, depth_add_f2, rgb_decoder_out_2 = self.fusion2(rgb_decoder_out_2, depth_decoder_out_2)

        # Level 1: Final fusion with intermediate segmentation
        rgb_decoder_out_1 = self.deconv1_rgb(rgb_decoder_out_2)
        rgb_seg_f1, depth_add_f1, rgb_decoder_out_1 = self.fusion1(rgb_decoder_out_1, depth_decoder_out_1)

        # Return all outputs for deep supervision and contrastive learning
        return (depth_en_out, rgb_en_out, depth_decoder_out_1, rgb_decoder_out_1, 
                rgb_seg_f1, depth_add_f1, rgb_seg_f2, depth_add_f2, rgb_seg_f3, depth_add_f3)


class LuSeg_S(nn.Module):
    """
    LuSeg Student Model (Simplified Dual-Encoder with Basic Fusion)
    
    Lighter version of LuSeg_T with simplified fusion strategy. Designed for:
        - Knowledge distillation from the teacher model
        - Faster inference with reduced computational cost
        - Deployment scenarios where efficiency is critical
    
    Key Differences from Teacher:
        - Simpler concatenation-based fusion (no FusionV10 modules)
        - Fewer output heads (only 3 vs 10 for teacher)
        - No intermediate segmentation outputs
    
    Input:
        4-channel tensor: [RGB (3 channels) | Thermal/Depth (1 channel)]
    
    Outputs:
        - depth_en_out: Depth encoder features (for contrastive loss)
        - rgb_en_out: RGB encoder features (for contrastive loss)
        - rgb_decoder_out_1: Final segmentation prediction
    
    Args:
        n_class: Number of segmentation classes
    """
    def __init__(self, n_class):
        super(LuSeg_S, self).__init__()
        
        # Load ResNet-50 backbones
        resnet_raw_model1 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        resnet_raw_model2 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        self.inplanes = 2048 

        # ========== THERMAL ENCODER ==========
        self.encoder_depth_conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.encoder_depth_conv1.weight.data = torch.unsqueeze(
            torch.mean(resnet_raw_model1.conv1.weight.data, dim=1), dim=1
        )
        self.encoder_depth_bn1 = resnet_raw_model1.bn1
        self.encoder_depth_relu = resnet_raw_model1.relu
        self.encoder_depth_maxpool = resnet_raw_model1.maxpool
        self.encoder_depth_layer1 = resnet_raw_model1.layer1
        self.encoder_depth_layer2 = resnet_raw_model1.layer2
        self.encoder_depth_layer3 = resnet_raw_model1.layer3
        self.encoder_depth_layer4 = resnet_raw_model1.layer4

        # ========== RGB ENCODER ==========
        self.encoder_rgb_conv1 = resnet_raw_model2.conv1
        self.encoder_rgb_bn1 = resnet_raw_model2.bn1
        self.encoder_rgb_relu = resnet_raw_model2.relu
        self.encoder_rgb_maxpool = resnet_raw_model2.maxpool
        self.encoder_rgb_layer1 = resnet_raw_model2.layer1
        self.encoder_rgb_layer2 = resnet_raw_model2.layer2
        self.encoder_rgb_layer3 = resnet_raw_model2.layer3
        self.encoder_rgb_layer4 = resnet_raw_model2.layer4

        # ========== DECODER (Shared/Fused Path) ==========
        self.skip_tranform_rgb = nn.Conv2d(in_channels=64, out_channels=128, 
                                          kernel_size=3, stride=1, padding=1)
        self.deconv5_rgb = upbolckV1(cin=2048, cout=1024)
        self.deconv4_rgb = upbolckV1(cin=1024, cout=512)
        self.deconv3_rgb = upbolckV1(cin=512, cout=256)
        self.deconv2_rgb = upbolckV1(cin=256, cout=128)
        self.deconv1_rgb = upbolckV1(cin=128, cout=n_class)

        self.skip_tranform_depth = nn.Conv2d(in_channels=64, out_channels=128, 
                                            kernel_size=3, stride=1, padding=1)
        # Note: No separate depth decoder in student model
        
        # ========== SIMPLIFIED FUSION MODULES ==========
        # Instead of complex FusionV10, use simple concatenation + 1x1 conv
        
        # Level 5: Concatenate (deconv_out + skip_rgb + skip_depth)
        # Channel count: 1024 + 1024 + 1024 = 3072
        self.decoder_cat1 = nn.Conv2d(in_channels=3072, out_channels=1024, 
                                     kernel_size=3, stride=1, padding=1)
        self.decoder_bn1 = nn.BatchNorm2d(1024)
        
        # Level 4: 512 + 512 + 512 = 1536
        self.decoder_cat2 = nn.Conv2d(in_channels=1536, out_channels=512, 
                                     kernel_size=3, stride=1, padding=1)
        self.decoder_bn2 = nn.BatchNorm2d(512)
        
        # Level 3: 256 + 256 + 256 = 768
        self.decoder_cat3 = nn.Conv2d(in_channels=768, out_channels=256, 
                                     kernel_size=3, stride=1, padding=1)
        self.decoder_bn3 = nn.BatchNorm2d(256)
        
        # Level 2: 128 + 128 + 128 = 384
        self.decoder_cat4 = nn.Conv2d(in_channels=384, out_channels=128, 
                                     kernel_size=3, stride=1, padding=1)
        self.decoder_bn4 = nn.BatchNorm2d(128)

    def forward(self, input):
        """
        Forward pass through the student model.
        
        Args:
            input: 4-channel tensor (batch, 4, H, W)
            
        Returns:
            depth_en_out: Flattened depth encoder features
            rgb_en_out: Flattened RGB encoder features
            rgb_decoder_out_1: Final segmentation (batch, n_class, H, W)
        """
        rgb = input[:, :3]
        depth = input[:, 3:]

        # ========== DEPTH ENCODER ==========
        depth = self.encoder_depth_conv1(depth)
        depth = self.encoder_depth_bn1(depth)
        depth = self.encoder_depth_relu(depth)
        skip_d_1 = depth
        
        depth = self.encoder_depth_maxpool(depth)
        depth = self.encoder_depth_layer1(depth)
        skip_d_2 = depth
        
        depth = self.encoder_depth_layer2(depth)
        skip_d_3 = depth
        
        depth = self.encoder_depth_layer3(depth)
        skip_d_4 = depth
        
        depth = self.encoder_depth_layer4(depth)
        depth_en_out = depth.view(depth.size(0), -1)

        # ========== RGB ENCODER ==========
        rgb = self.encoder_rgb_conv1(rgb)
        rgb = self.encoder_rgb_bn1(rgb)
        rgb = self.encoder_rgb_relu(rgb)
        skip_r_1 = rgb
        
        rgb = self.encoder_rgb_maxpool(rgb)
        rgb = self.encoder_rgb_layer1(rgb)
        skip_r_2 = rgb
        
        rgb = self.encoder_rgb_layer2(rgb)
        skip_r_3 = rgb
        
        rgb = self.encoder_rgb_layer3(rgb)
        skip_r_4 = rgb
        
        rgb = self.encoder_rgb_layer4(rgb)
        rgb_en_out = rgb.view(rgb.size(0), -1)

        # ========== FUSED DECODER (Simplified) ==========
        # Start: Simple addition of encoder outputs
        out = rgb + depth
        
        # Level 5: Upsample and fuse with skip connections from both encoders
        rgb_decoder_out_5 = self.deconv5_rgb(out)
        rgb_decoder_out_5 = F.relu(self.decoder_bn1(
            self.decoder_cat1(torch.cat((rgb_decoder_out_5, skip_r_4, skip_d_4), dim=1))
        ))

        # Level 4
        rgb_decoder_out_4 = self.deconv4_rgb(rgb_decoder_out_5)
        rgb_decoder_out_4 = F.relu(self.decoder_bn2(
            self.decoder_cat2(torch.cat((rgb_decoder_out_4, skip_r_3, skip_d_3), dim=1))
        ))

        # Level 3
        rgb_decoder_out_3 = self.deconv3_rgb(rgb_decoder_out_4)
        rgb_decoder_out_3 = F.relu(self.decoder_bn3(
            self.decoder_cat3(torch.cat((rgb_decoder_out_3, skip_r_2, skip_d_2), dim=1))
        ))
        
        # Level 2
        rgb_decoder_out_2 = self.deconv2_rgb(rgb_decoder_out_3)
        skip_r_1 = self.skip_tranform_rgb(skip_r_1)
        skip_d_1 = self.skip_tranform_depth(skip_d_1)
        rgb_decoder_out_2 = F.relu(self.decoder_bn4(
            self.decoder_cat4(torch.cat((rgb_decoder_out_2, skip_r_1, skip_d_1), dim=1))
        ))
        
        # Level 1: Final segmentation output
        rgb_decoder_out_1 = self.deconv1_rgb(rgb_decoder_out_2)
        
        return depth_en_out, rgb_en_out, rgb_decoder_out_1 


class LuSeg_RGB(nn.Module):
    """
    LuSeg RGB-Only Model (Single-Modality Variant)
    
    Simplified version that processes only RGB images, without thermal/depth input.
    Useful for:
        - Scenarios where thermal data is unavailable
        - Baseline comparisons to measure thermal contribution
        - Single-sensor deployment
    
    Architecture:
        - Single ResNet-50 encoder for RGB
        - U-Net style decoder with skip connections
        - Concatenation-based feature fusion
    
    Input:
        3-channel RGB tensor (batch, 3, H, W)
    
    Outputs:
        - rgb_en_out: Encoder features (for contrastive loss if needed)
        - rgb_decoder_out_1: Final segmentation prediction
    
    Args:
        n_class: Number of segmentation classes (default=5)
    """
    def __init__(self, n_class=5):
        super(LuSeg_RGB, self).__init__()
        
        # Load ResNet-50 backbone
        resnet_raw_model2 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        self.inplanes = 2048

        # ========== RGB ENCODER ==========
        self.encoder_rgb_conv1 = resnet_raw_model2.conv1
        self.encoder_rgb_bn1 = resnet_raw_model2.bn1
        self.encoder_rgb_relu = resnet_raw_model2.relu
        self.encoder_rgb_maxpool = resnet_raw_model2.maxpool
        self.encoder_rgb_layer1 = resnet_raw_model2.layer1
        self.encoder_rgb_layer2 = resnet_raw_model2.layer2
        self.encoder_rgb_layer3 = resnet_raw_model2.layer3
        self.encoder_rgb_layer4 = resnet_raw_model2.layer4

        # ========== DECODER ==========
        self.skip_tranform_rgb = nn.Conv2d(in_channels=64, out_channels=128, 
                                          kernel_size=3, stride=1, padding=1)
        self.deconv5_rgb = upbolckV1(cin=2048, cout=1024)
        self.deconv4_rgb = upbolckV1(cin=1024, cout=512)
        self.deconv3_rgb = upbolckV1(cin=512, cout=256)
        self.deconv2_rgb = upbolckV1(cin=256, cout=128)
        self.deconv1_rgb = upbolckV1(cin=128, cout=n_class)

        # ========== CONCATENATION LAYERS (RGB-Only) ==========
        # Only concatenate decoder output with RGB skip connections
        
        # Level 5: 1024 + 1024 = 2048
        self.decoder_cat1 = nn.Conv2d(in_channels=2048, out_channels=1024, 
                                     kernel_size=3, stride=1, padding=1)
        self.decoder_bn1 = nn.BatchNorm2d(1024)
        
        # Level 4: 512 + 512 = 1024
        self.decoder_cat2 = nn.Conv2d(in_channels=1024, out_channels=512, 
                                     kernel_size=3, stride=1, padding=1)
        self.decoder_bn2 = nn.BatchNorm2d(512)
        
        # Level 3: 256 + 256 = 512
        self.decoder_cat3 = nn.Conv2d(in_channels=512, out_channels=256, 
                                     kernel_size=3, stride=1, padding=1)
        self.decoder_bn3 = nn.BatchNorm2d(256)
        
        # Level 2: 128 + 128 = 256
        self.decoder_cat4 = nn.Conv2d(in_channels=256, out_channels=128, 
                                     kernel_size=3, stride=1, padding=1)
        self.decoder_bn4 = nn.BatchNorm2d(128)

    def forward(self, input):
        """
        Forward pass for RGB-only model.
        
        Args:
            input: 3-channel RGB tensor (batch, 3, H, W)
            
        Returns:
            rgb_en_out: Flattened encoder features
            rgb_decoder_out_1: Final segmentation (batch, n_class, H, W)
        """
        rgb = input  # Input is 3-channel RGB image

        # ========== RGB ENCODER ==========
        rgb = self.encoder_rgb_conv1(rgb)
        rgb = self.encoder_rgb_bn1(rgb)
        rgb = self.encoder_rgb_relu(rgb)
        skip_r_1 = rgb

        rgb = self.encoder_rgb_maxpool(rgb)
        rgb = self.encoder_rgb_layer1(rgb)
        skip_r_2 = rgb

        rgb = self.encoder_rgb_layer2(rgb)
        skip_r_3 = rgb

        rgb = self.encoder_rgb_layer3(rgb)
        skip_r_4 = rgb

        rgb = self.encoder_rgb_layer4(rgb)
        rgb_en_out = rgb.view(rgb.size(0), -1)

        # ========== RGB DECODER ==========
        out = rgb
        
        # Level 5
        rgb_decoder_out_5 = self.deconv5_rgb(out)
        rgb_decoder_out_5 = F.relu(self.decoder_bn1(
            self.decoder_cat1(torch.cat((rgb_decoder_out_5, skip_r_4), dim=1))
        ))

        # Level 4
        rgb_decoder_out_4 = self.deconv4_rgb(rgb_decoder_out_5)
        rgb_decoder_out_4 = F.relu(self.decoder_bn2(
            self.decoder_cat2(torch.cat((rgb_decoder_out_4, skip_r_3), dim=1))
        ))

        # Level 3
        rgb_decoder_out_3 = self.deconv3_rgb(rgb_decoder_out_4)
        rgb_decoder_out_3 = F.relu(self.decoder_bn3(
            self.decoder_cat3(torch.cat((rgb_decoder_out_3, skip_r_2), dim=1))
        ))
        
        # Level 2
        rgb_decoder_out_2 = self.deconv2_rgb(rgb_decoder_out_3)
        skip_r_1 = self.skip_tranform_rgb(skip_r_1)
        rgb_decoder_out_2 = F.relu(self.decoder_bn4(
            self.decoder_cat4(torch.cat((rgb_decoder_out_2, skip_r_1), dim=1))
        ))
        
        # Level 1: Final segmentation
        rgb_decoder_out_1 = self.deconv1_rgb(rgb_decoder_out_2)
        
        return rgb_en_out, rgb_decoder_out_1 


class LuSeg_Depth(nn.Module):
    """
    LuSeg Depth/Thermal-Only Model (Single-Modality Variant)
    
    Processes only thermal or depth images without RGB input. Useful for:
        - Night-time operation (thermal-only)
        - Depth-based segmentation tasks
        - Ablation studies to measure depth/thermal contribution
    
    Architecture:
        - Single ResNet-50 encoder adapted for 1-channel input
        - U-Net style decoder with skip connections
        - Simpler than dual-encoder variants
    
    Input:
        1-channel thermal/depth tensor (batch, 1, H, W)
    
    Outputs:
        - depth_en_out: Encoder features
        - depth_decoder_out_1: Final segmentation prediction
    
    Args:
        n_class: Number of segmentation classes
    """
    def __init__(self, n_class):
        super(LuSeg_Depth, self).__init__()
        
        # Load ResNet-50 backbone
        resnet_raw_model1 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        self.inplanes = 2048

        # ========== THERMAL/DEPTH ENCODER ==========
        # Adapt first conv layer for 1-channel input
        self.encoder_depth_conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
        # Initialize weights by averaging RGB conv1 weights across input channels
        self.encoder_depth_conv1.weight.data = torch.unsqueeze(
            torch.mean(resnet_raw_model1.conv1.weight.data, dim=1), dim=1
        )
        
        self.encoder_depth_bn1 = resnet_raw_model1.bn1
        self.encoder_depth_relu = resnet_raw_model1.relu
        self.encoder_depth_maxpool = resnet_raw_model1.maxpool
        self.encoder_depth_layer1 = resnet_raw_model1.layer1
        self.encoder_depth_layer2 = resnet_raw_model1.layer2
        self.encoder_depth_layer3 = resnet_raw_model1.layer3
        self.encoder_depth_layer4 = resnet_raw_model1.layer4

        # ========== DECODER ==========
        self.skip_tranform_depth = nn.Conv2d(in_channels=64, out_channels=128, 
                                            kernel_size=3, stride=1, padding=1)
        self.deconv5_depth = upbolckV1(cin=2048, cout=1024)
        self.deconv4_depth = upbolckV1(cin=1024, cout=512)
        self.deconv3_depth = upbolckV1(cin=512, cout=256)
        self.deconv2_depth = upbolckV1(cin=256, cout=128)
        self.deconv1_depth = upbolckV1(cin=128, cout=n_class)

    def forward(self, input):
        """
        Forward pass for depth/thermal-only model.
        
        Args:
            input: 1-channel depth/thermal tensor (batch, 1, H, W)
            
        Returns:
            depth_en_out: Flattened encoder features
            depth_decoder_out_1: Final segmentation (batch, n_class, H, W)
        """
        depth = input  # Input is 1-channel depth/thermal image

        # ========== DEPTH ENCODER ==========
        depth = self.encoder_depth_conv1(depth)
        depth = self.encoder_depth_bn1(depth)
        depth = self.encoder_depth_relu(depth)
        skip_d_1 = depth
        
        depth = self.encoder_depth_maxpool(depth)
        depth = self.encoder_depth_layer1(depth)
        skip_d_2 = depth
        
        depth = self.encoder_depth_layer2(depth)
        skip_d_3 = depth
        
        depth = self.encoder_depth_layer3(depth)
        skip_d_4 = depth
        
        depth = self.encoder_depth_layer4(depth)
        depth_en_out = depth.view(depth.size(0), -1)

        # ========== DEPTH DECODER ==========
        # U-Net decoder with skip connections from encoder
        depth_decoder_out_5 = self.deconv5_depth(depth) + skip_d_4
        depth_decoder_out_4 = self.deconv4_depth(depth_decoder_out_5) + skip_d_3
        depth_decoder_out_3 = self.deconv3_depth(depth_decoder_out_4) + skip_d_2
        depth_decoder_out_2 = self.deconv2_depth(depth_decoder_out_3) + self.skip_tranform_depth(skip_d_1)
        depth_decoder_out_1 = self.deconv1_depth(depth_decoder_out_2)

        return depth_en_out, depth_decoder_out_1


# ============================================================================
# USAGE EXAMPLES AND NOTES
# ============================================================================
"""
Model Selection Guide:
----------------------

1. LuSeg_T (Teacher Model):
   - Use when: Maximum accuracy is required, dual-modality data available
   - Best for: Training and knowledge distillation
   - Input: 4-channel (RGB + Thermal/Depth)
   - Outputs: 10 tensors (includes deep supervision signals)

2. LuSeg_S (Student Model):
   - Use when: Efficiency matters, dual-modality data available
   - Best for: Real-time inference, embedded deployment
   - Input: 4-channel (RGB + Thermal/Depth)
   - Outputs: 3 tensors
   - ~30-40% faster than teacher with ~2-5% accuracy drop

3. LuSeg_RGB:
   - Use when: Only RGB data available
   - Best for: Daytime operation, standard cameras
   - Input: 3-channel RGB
   - Outputs: 2 tensors

4. LuSeg_Depth:
   - Use when: Only thermal/depth data available
   - Best for: Night-time operation, LIDAR-based systems
   - Input: 1-channel Depth/Thermal
   - Outputs: 2 tensors

Training Tips:
--------------
- Use teacher model outputs for knowledge distillation to student
- Apply deep supervision losses to intermediate outputs (rgb_seg_f1-f3, depth_add_f1-f3)
- Use NTXentLoss on encoder outputs (depth_en_out, rgb_en_out) for contrastive learning
- Consider freezing encoder layers initially for faster convergence
- Recommended input size: 256x256 or 512x512 (must be divisible by 32)

Example Usage:
--------------
# Initialize model
model = LuSeg_T(n_class=5)  # For 5-class segmentation
model = model.cuda()

# Forward pass
input_4ch = torch.randn(8, 4, 256, 256).cuda()  # Batch of 8 images
outputs = model(input_4ch)

# Unpack outputs
depth_enc, rgb_enc, depth_seg, rgb_seg, seg_f1, add_f1, seg_f2, add_f2, seg_f3, add_f3 = outputs

# Main segmentation prediction
pred = rgb_seg  # Shape: (8, 5, 256, 256)
"""
